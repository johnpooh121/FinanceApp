# FinanceApp

## 개요

국내의 주가 정보를 수집하고 DB에 저장해서 API로 제공

서비스 링크 : https://home.finance-app.site/

프론트엔드 : https://github.com/johnpooh121/FinanceApp-next/

### 동기

- 1년 동안 실무에서 배운 지식들을 내 앱에 적용시켜보고 싶어짐
- 저가매수 전략을 도와주는 알림 시스템이 있으면 좋겠다고 생각

### 목표

- aws 서버를 띄워서 주기적으로 데이터를 업데이트하는 기능을 구현
- DB를 활용해서 데이터 저장, 쿼리 기능 구현
- 저가매수 전략을 위한 이메일 알림 시스템 구현

## 2. 기능

### 2.1. 카카오 로그인 & 인증

- 유저는 카카오 OAuth를 통해 별도의 개인정보 제공 없이 회원가입 / 로그인을 할 수 있음
- 유저 시스템은 후에 설명할 다운로드 API의 남용을 막기 위함

### 2.2. 데이터 다운로드

<img src=images/README_2025-05-27_20-51-30.png width=65%>

- 원하는 종목과 조회 시작일, 종료일을 입력하면 해당하는 데이터를 다운받을 수 있다
- 데이터의 각 행은 종목, 일자, 종가, PER, 시가총액 등으로 구성됨
- 각 유저는 월 별로 4백만 행까지 다운로드 받을 수 있음

### 2.3. 이메일 알림 설정

<img src=images/README_2025-05-27_20-54-25.png width=65%>

- PER, 배당 수익률 등에 대해 나만의 조건을 설정하면 추후에 조건에 해당하는 종목이 있을 때 이메일로 알림을 받을 수 있음
- 또한 현재 설정한 조건에 맞는 종목이 있는지 바로 조회해볼 수 있다

## 3. 세부 구현

### 3.1. 시스템 구조

띄울 웹 서버에 cron 기능까지 추가하면 cron time 관련해서 배포 후 동적으로 바꾸기 어렵기 때문에 cronjob을 트리거하는 주체를 웹 서버와 분리함

**구성**

- 메인 api 서버인 t3.micro EC2 인스턴스
- 메인서버의 크론잡을 호출하는 용도의 Lambda 함수
- RDS (MySQL Comunity) t4g.micro 인스턴스 1개
- 위의 Lambda 함수를 트리거하는 EventBridge Schedule 3개 (유저 다운로드 쿼터 리셋, 일별 데이터 크롤링, 일별 이메일 발송)
- 프론트는 nextjs로 구현 후 vercel로 배포

### 3.2. 데이터 크롤링

- KOSPI와 KOSDAQ 의 모든 상장 주식 데이터를 크롤링
- 데이터 수집 출처 : [KRX 정보 데이터시스템](http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201020101)
- 수집 데이터 : 일자 & 종목별 가격정보, 시가총액, 거래량, 거래대금, 주식 수, 배당률, PER, PBR, EPS, BPS, 외국인 보유량 등
- 일자 범위 : 2005년 1월 3일부터 현재까지
- 총 데이터 크기 : 9백만개에서 증가중

<img src=images/image.png width=65%>

<img src=images/image-1.png width=65%>

- 맨 처음에 DB가 비어있을 때 /crawl/init 으로 데이터를 채움
- 그 후 매일 AWS EventBridge로 lambda를 invoke 시키고 lambda 함수에서 서버로 /crawl/daily를 호출해서 일일 데이터를 채움
- 주식의 종가는 모두 수정종가
- 액면분할을 감지하기 위해 일일 크롤링 후 전날 주식 `종가`가 오늘 데이터의 `종가` - `대비`와 맞지 않는 주식을 찾음
- 액면분할 감지 시 해당 종목에 대해서 개별 크롤링을 진행, 그 종목의 모든 데이터 업데이트

### 3.3. 유저 로그인 및 보안

- 유저 로그인 : 카카오 OAuth를 이용해서 구현
- 로그인 시 Bearer 토큰과 리프레시 토큰, 액세스 토큰을 발급
- Bearer 토큰은 로컬 스토리지, refresh와 access 토큰은 HTTP only 쿠키에 저장함으로써 XSS와 CSRF 공격을 방지
- 유저 api 이외의 모든 api들은 API key 방식으로 Admin 인증을 진행
- 가비아에서 도메인을 구매하고 aws certificate manager로 백엔드 서버에 https를 적용

### 3.4. 데이터 다운로드

- AWS Free Tier이기 때문에 과금 방지를 위해 유저별로 월 별 다운로드 가능 row 수에 제한을 둠
- 메모리 부족을 막기 위해 DB에서 전체 데이터를 받는 대신 stream으로 HTTP response로 보내는 방식으로 구현

### 3.5. 이메일 구독 기능

- 유저가 수신 이메일 주소와 알림 조건을 설정하면, 조건에 부합하는 종목이 있을 시 이메일로 알림을 발송
- 조건은 시가총액, PER, PBR, EPS, 배당수익률, 52주 최고가 대비 하락률, 52주 최저가 대비 상승률 등의 지표에 대해 설정 가능
- 52주 최저가와 52주 최고가는 공공데이터로 제공되지 않기 때문에 쿼리 속도를 위해 미리 계산해서 저장해둠
- 이메일 발송을 위해 테스트 구글 계정을 만들고 OAuth를 이용해서 최초 refresh token을 받아 계속 사용

### 3.6. 웹 페이지

- 처음에는 이왕 하는거 리액트나 vue를 공부해볼까 싶었지만, 시간도 부족하고 AWS Free Tier에서 프론트엔드용 인스턴스를 새로 띄우거나 t3.micro에 api서버와 프론트엔드를 같이 실행하긴 부담이 클 것 같았다
- 그래서 일단 nestjs에서 지원하는 HandleBars 라는 간단한 컨텐츠 호스팅 기능을 이용해서 api 서버와 웹페이지 서버를 같이 구현하였다 (구현 링크 : https://api.finance-app.site/web/login)
- 그러나 순수 html의 한계로 페이지 가독성이 너무 안좋기도 하고, 백엔드 개발만 하다보니 프론트엔드는 어떻게 구현하는지 궁금했어서 맛보기로 vercel + NextJs 기반 프론트엔드를 개발하였다
- 다행히 미리 짜둔 순수 html + js 코드를 기반으로 chatgpt의 도움을 받아 빠르게 현대적인 UI의 프론트엔드를 개발할 수 있었다

## 4. 겪었던 시행착오

### 4.1. 크롤링 방식

- 처음에는 ip 차단 우회를 위해서 ip를 풀에서 동적으로 할당받는 aws lambda로 크롤링 기능을 구현하고, EventBridge로 주기적으로 호출하려고 했음
- 하지만 추가비용 없이 lambda에서 얻은 데이터를 DB에 저장하기 위해서는 lambda를 vpc에 연결하거나, 인터넷을 통해서 api 서버를 거치게 하는 방법, 데이터 교환이 무료인 DynamoDB나 S3 등을 쓰는 방법이 있는데, 첫번째는 ip 풀의 이점이 사라지고, 나머지 방법은 구현이 복잡해짐
- 그래서 그냥 크롤링 기능은 api 서버에 구현하였음

### 4.2. 52주 최저가와 최고가 구하기

- 처음에는 아무 생각 없이 각 일자에 대해서 52주 치 데이터를 보고 선형으로 최고가와 최저가를 구하는 알고리즘으로 생각함
- 근데 더 개선할 수 있을 것 같아서 고민하다가 이전에 취미로 풀었던 USACO 2006 Bad Hair Day 문제의 스택 풀이에서 영감을 받아서 dequeue로 구현하였다
- 간단한 알고리즘 요약 (52주 최저가 구하기)
  - 일자를 ascending order로 순회하면서 아래를 반복
    - 먼저 dequeue(이하 덱)의 front를 보면서 (현재 일자 - 1년) 보다 이전 데이터가 있으면 pop한다
    - 그 후 오늘 일자의 데이터를 덱의 back에 넣는데, 현재 덱의 back의 가격이 오늘 가격보다 높으면 pop한다 (반복)
    - 그렇게 pop하다가 덱이 비거나 덱의 back의 가격이 오늘 가격 이하일 때 덱의 back에 오늘 가격을 push한다
    - 오늘 일자에 대한 52주 최저가는 덱의 front의 가격이 된다.
- 소스코드는 웹 서버 repository의 src/data/data.service.ts에 있다

## 5. 배운점

직접 개인 AWS 환경에서 모든 인프라를 구축해보면서 어렴풋이만 알던 AWS를 더 디테일하게 알 수 있었다.

나름 규모 있는 프로젝트를 하면서 단순히 코드를 잘 구현하는 것 외에도 인프라 설계에도 비용, 효율 등 신경쓸 것들이 많다는 것을 느꼈다.

회사에서 별 생각없이 쓰던 nestjs, typeorm 등의 프레임워크, 기술에 대해 자세히 알 수 있었고, 백엔드 직무에서 다룰 일이 없던 html+plain js, Nextjs도 맛보기로 다뤄보면서 웹 페이지가 어떻게 동작하는지 이해를 더 높일 수 있었다.

## 6. TODO

미국증시 데이터 수집기능
